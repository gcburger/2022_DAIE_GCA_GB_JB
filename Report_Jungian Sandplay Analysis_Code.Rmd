--- 
title: "Descriptive & Inferential Analysis of a Jungian Sandplay VR Project"
author: "Gareth Burger &amp; James Bunt"
date: "`r format(Sys.time(), '%a, %d %B %y at %X')`"
always_allow_html: true
output:
  html_document:
    toc: yes
    toc_title: Table Of Contents
    toc_float: yes
    toc_depth: 3
    fig_caption: no
    lof: yes
    number_sections: yes
    code_folding: hide
    theme: paper
  pdf_document:
    toc: true
    toc_depth: '3'
fontsize: 10pt
subtitle: "Normality testing of patient treatment - Code"
---

```{r setup and library load, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
cat('\014')
```

**WIP**\
*We can rename this README or make the main R Notebook file a completely new file if needed*\
*2-2.5 pages of text needed, not more than 1200 words excluding plots and tables*\

---

# Abstract

Based on the data provided of an experiment with 149 patients (1 removed) to test the effectiveness of VR treatment on PTSD, it is reasonable to conclude that the VR treatment (both animated and static) had a positive effect on PTSD, regardless of gender.

The experiment can be deemed as reliable since we have a randomised controlled trial where participants within a pre-determined criteria (young adults aged 18-25 years old) were sourced using random sampling and randomly assigned to one of three test groups (1 control group and 2 different types of VR).

Observer reviewed test results were only considered since participant self assessments are unreliable. Results comparing the pre and post PTSD scores after VR treatment yielded a 15.01% improvement compared to the control group's improvement of 11.25% (+3.8%). Whilst all treatment provided an overall improvement, a null hypothesis that VR treatment does not impact PTSD can be rejected and an alternative hypothesis that VR treatment will result in a greater reduction in PTSD compared to traditional pyshchological therapy (control group) is suggested.

It is important to keep in mind that the above hypothesis is a prediction and not a definitive statement about the relationship between the VR treatment and reduced PTSD, but rather a starting point for further investigation where more information is needed. Whilst the data meets the assumptions required for a statistical test, it should also be noted that the experiment information available is limited which limits the reliability of findings. Specifically, the age of each participant (i.e. there maybe additional influence of age), the amount of time since diagnosis (since both HSE and Mayo Clinic advise that some PTSD can naturally reduce over time), whether the patient is receiving any potentially conflicting treatment (i.e. PTSD medication) and the time of observation (it has been noted that observations are recorded at the start and end of each day however this time of observation is not included in the data provided) can impact the reliability of the experiment which can not be validated at this time.

---

*NB to be succinct and clear here*

## Aim and rationale

## Participants and setting

## Experiment design

## Results gathering

## Major findings

## Findings/implications

---

# Introduction

---

According to the HSE and Mayo Clinic (2022) Post-traumatic stress disorder (PTSD) is a mental health condition that is triggered by a terrifying event â€” either experiencing it or witnessing it. Symptoms may include flashbacks, nightmares and severe anxiety, as well as uncontrollable thoughts about the event (Mayo Clinic, 2022). PTSD symptoms can be physical and/ or emotional and can vary over time or vary from person to person (with or without treatment).

Treatment for PTSD can vary between psychological therapies (such as cognitive behavioural therapy - CBT) and medication (such as antidepressants). This study will only focus on the difference between traditional pyscholigical therapy (CBT) and new VR versions (animated and static) of a Jungian Sandplay  (a therapeutic method that uses sand, miniature objects, and image making within the context of the psychology of Carl Jung (Maeve Dooley, October 2018).

There are several methods employed to assess patient PTSD including observer-rated (OR), self- reported (CPSS), parental reported, and physiological measurements however only OR pre and post scores are used in this analysis due to reliability concerns of self recorded data outside a controlled and standardised manner which is essential for a robust experiment. Also note that specific focus will be applied on the variance between the pre and post scores (impact).

## Topic and context

## Rationale

*2. Formulate a single hypothesis test to be used to compare the effectiveness of the approaches used during the experiment.*\

## Hypothesis

Null hypothesis is that VR treatments do not have a greater effect on PTSD than the control group (tranditional CBT therapy) with an alternative hypothesis that both animated and static VR treatments have a positive effect on PTSD.

One possible hypothesis test that could be used to compare the effectiveness of the different approaches used in the experiment is a two-tailed independent samples t-test. This test could be used to compare the mean change in PTSD levels (as measured by the pre-trial and post-trial CPSS and OR scores) between the Static and Animated groups, with the null hypothesis being that there is no significant difference between the two groups. This test would help determine if the use of animated VR content leads to significantly greater reductions in PTSD levels than the use of static VR content.

The study consisted of 150 patients (evenly split between male and female) divided into the groups listed below, using random sampling. A population of 149 patients were analysed (1 removed due to missing data) with 99 recieving treatment and 50 patients in the control group. All patients were young adults in the age range of 18 - 25 years however information on the exact age was not recorded but gender information was recorded. 

Note that there is no confirmation that participants are not receiving any additional treatment which can easily impact the experiment results therefore the hypothesis can not be validated

---

# Method

---

## Participants

## Design

There are a few important factors that should be considered when conducting this experiment which has not been explicitly mentioned so must be considered and noted as it impacts the reliability of analysis and results.

Firstly, it is important to ensure that the study is conducted in a controlled and standardized manner. This means that the sample of patients should be selected using a random sampling method, and the groups should be of equal size to avoid any potential bias. From the information provided, since an appropriate random sampling approach has been followed, this is deemed to be appropriate and reliable however it should be noted that we have no information regarding other treatment which the patients could be involved with (since all patients have diagnosed PTSD, treatment is likely) which can directly impact the reliability of these results.

Second, studies like this should be designed to control for potential confounders, such as the patient's age, gender, and any other factors that could affect the outcome of the experiment. Gender and age is controlled within sampling which is sufficient however additional information on potentially conflicting treatment (i.e. mediciation) is not mentioned. Within the data, whilst gender is recorded, information on specific age and time of observation (i.e. start of end of day) which can impact results. 

Third, it is important that this study should use valid and reliable measures to assess the patients' PTSD levels, both at the start and end of the study therefore all self recorded (CPSS) results have been discounted since they are self reported and uncontrolled. Focusing analysis on the observer rated results (OR) will help ensure that the results of the study are accurate and unbiased so they can be compared across the different test groups.

Fourth, it should be noted that it is important to ensure that the therapists administering the treatment are trained and experienced in using the VR app, as well as in providing traditional CBT which has not been validated within the information provided. It is assumed that all patients are receiving high-quality, consistent treatment so that any differences in outcomes between the groups can be attributed to the difference in treatment type.

Finally, the study should include a sufficient number of patients to provide statistical power and to ensure that the results are statistically significant. It has been deemed that the 150 patient population (50 per treatment type) is sufficient for this analysis and will help ensure that any observed differences between the groups can be confidently attributed to the VR app rather than to chance.

## Materials

## Procedure

### Load Data
```{r Load Data, echo=TRUE}
# load data from csv
file_name = "./data/daie_ca3_data_13.csv"
study_data <- read.csv(file_name)
```

### Inspect Data Structure
```{r Inspect Head, Tail & Structure, echo=TRUE}
# show data structure
#str(study_data)

# show first 10 rows
#head(study_data, 10)

# show last 10 rows
#tail(study_data, 10)

# show a paged table containing all the data
rmarkdown::paged_table(study_data)
```

### Data Cleaning and Filtering
There are three instances of error in the data:

- Row 14: post_trial_cpss has a value of 12 (test_group = Static)
- Row 42: test_group is missing value 'Static'
- Row 64: gender has a value of 'mal'

Our hypothesis is looking at the difference between pre_trial_or and post_trial_or scores for the Control and Animated groups only, not testing gender as a contributing factor, so as it turned out we can safely ignore the errors in the data as they do not form part of the filtered subset we are investigating.

```{r}
# install.packages("tidyverse")
# install.packages("dplyr")

# load and attach tidyverse and dplyr packages
library(tidyverse)
library(dplyr)

View(study_data)

# variable types
#glimpse(study_data)
```

#### Filtered Subset
```{r}
# select filtered data required for hypothesis testing
study_data_filtered <- study_data %>%
  select(test_group, ends_with("or")) %>%
  filter(test_group %in% c("Control", "Animated") 
         & pre_trial_or > 0, pre_trial_or < 10
         & post_trial_or > 0, post_trial_or < 10) %>%
  mutate(trial_or_diff = post_trial_or - pre_trial_or)

subset_control <- study_data_filtered %>%
  select(test_group, trial_or_diff) %>%
  filter(test_group == "Control")

subset_animated <- study_data_filtered %>%
  select(test_group, trial_or_diff) %>%
  filter(test_group == "Animated")

# check for duplicates
study_data_filtered %>% distinct() # 100 rows returned, so no duplicates
duplicated(study_data_filtered) # all logical vector values are FALSE i.e. no duplicates

# group count check - Control and Animated both have 50 rows so no missing rows
study_data_filtered %>% count(test_group)

# check for missing score data
# all means are able to be calculated, thus there's no missing numeric data in those columns
mean(study_data_filtered$pre_trial_or) # 5.9741
mean(study_data_filtered$post_trial_or) # 5.3965
mean(study_data_filtered$trial_or_diff) # -0.5776

# extra check for completeness of rows: 0 incomplete rows in subset
study_data_filtered  %>%
  filter(!complete.cases(.))

# manually view and verify filtered data subset
View(study_data_filtered)
View(subset_control)
View(subset_animated)
```

---

# Results

---

## Descriptive statistics

### Mean and Standard Deviation
```{r}
mean_control_or_diff <- mean(subset_control$trial_or_diff)
std_control_or_diff <- sd(subset_control$trial_or_diff)

mean_animated_diff <- mean(subset_animated$trial_or_diff)
std_animated_or_diff <- sd(subset_animated$trial_or_diff)
```
control_or_diff: M = `r round(mean_control_or_diff, 2)`, SD = `r round(std_control_or_diff, 2)`

animated_or_diff: M = `r round(mean_animated_diff, 2)`, SD = `r round(std_animated_or_diff, 2)`

### Data Summary
A summary of the data is shown below:

```{r Summary, echo=TRUE}
summary(subset_control)
boxplot(subset_control$trial_or_diff, ylab = "Score", main = "Control Group OR Difference")

summary(subset_animated)
boxplot(subset_animated$trial_or_diff, ylab = "Score", main = "Animated Group OR Difference")
```

---

## Inferential statistics
The inference and statistical tests that we are doing on our data make the assumption that the data are normally distributed. Thus the below tests are first carried out to check this assumption by testing for normality.

### Testing for Normality

#### Method 1: Using Histogram / Density Plot
```{r Histograms, echo=TRUE}
# draw histogram for pre_trial_or
hist(study_data_filtered$pre_trial_or, main="Pre Trial Observer Rated", xlab="Measured Score", ylab="Frequency", breaks=6, col='steelblue')

# draw histogram for post_trial_or
hist(study_data_filtered$post_trial_or, main="Post Trial Observer Rated", xlab="Measured Score", ylab="Frequency", breaks=6, col='steelblue')

# draw histogram for control_or_diff
hist(subset_control$trial_or_diff, main="Control Group OR Difference", xlab="Score", ylab="Frequency", breaks=6, col='steelblue')

# draw histogram for animated_or_diff
hist(subset_animated$trial_or_diff, main="Animated Group OR Difference", xlab="Score", ylab="Frequency", breaks=6, col='steelblue')
```

Both histograms above exhibit data that is normally distributed as they are both roughly "bell-shaped" in nature.

---

#### Method 2: Using Q-Q Plot

```{r QQ Plots, echo=TRUE}
# perform qqnorm test on pre_trial_or
qqnorm(study_data_filtered$pre_trial_or, pch=2, frame=FALSE, main="Pre Trial Observer Rated")
qqline(study_data_filtered$pre_trial_or, col="darkorchid", lwd=1)

# perform qqnorm test on post_trial_or
qqnorm(study_data_filtered$post_trial_or, pch=2, frame=FALSE, main="Post Trial Observer Rated")
qqline(study_data_filtered$post_trial_or, col="darkorchid", lwd=1)

# perform qqnorm test on control_or_diff
qqnorm(subset_control$trial_or_diff, pch=2, frame=FALSE, main="Control Group OR Difference")
qqline(subset_control$trial_or_diff, col="darkorchid", lwd=1)

# perform qqnorm test on animated_or_diff
qqnorm(subset_animated$trial_or_diff, pch=2, frame=FALSE, main="Animated Group OR Difference")
qqline(subset_animated$trial_or_diff, col="darkorchid", lwd=1)
```

In both Q-Q plots above, the sample points roughly fall along the diagonal Q-Q line and thus give a good indication that the data is normally distributed. However, as not all points fall exactly on the line we will continue testing to investigate further.

---

#### Method 3: Statistical Test: Shapiro-Wilk
The threshold for normality is 0.05 (5%). If the p-value is above the threshold then we can accept the data to be normally distributed.\

```{r Shapiro-Wilk - pre_trial_or, echo=TRUE}
# perform shapiro-wilk test on pre_trial_or
pre_trial_or_sw <- shapiro.test(study_data_filtered$pre_trial_or)
pre_trial_or_sw
```
A `r pre_trial_or_sw$method` was conducted on the pre_trial_or data.\
From the output obtained we can assume normality as the p-value (p = `r round(pre_trial_or_sw$p.value, 4)`) is greater than 0.05.

```{r Shapiro-Wilk - post_trial_or, echo=TRUE}
# perform shapiro-wilk test on post_trial_or
post_trial_or_sw <- shapiro.test(study_data_filtered$post_trial_or)
post_trial_or_sw
```
A `r post_trial_or_sw$method` was conducted on the post_trial_or data.\
From the output obtained we can assume normality as the p-value (p = `r round(post_trial_or_sw$p.value, 4)`) is greater than 0.05.

```{r Shapiro-Wilk - control_or_diff, echo=TRUE}
# perform shapiro-wilk test on control_or_diff
control_or_diff_sw <- shapiro.test(subset_control$trial_or_diff)
control_or_diff_sw
```
A `r control_or_diff_sw$method` was conducted on the trial_or_diff data of the Control group.\
From the output obtained we can assume normality as the p-value (p = `r round(control_or_diff_sw$p.value, 4)`) is greater than 0.05.

```{r Shapiro-Wilk - animated_or_diff, echo=TRUE}
# perform shapiro-wilk test on animated_or_diff
animated_or_diff_sw <- shapiro.test(subset_animated$trial_or_diff)
animated_or_diff_sw
```
A `r animated_or_diff_sw$method` was conducted on the trial_or_diff data of the Animated group.\
From the output obtained we can assume normality as the p-value (p = `r round(animated_or_diff_sw$p.value, 4)`) is greater than 0.05.

As neither p-value is close to the threshold for normality, it is not necessary to test for normality via the fourth method of performing a Kolmogorov-Smirnov test.\

---

## Statistical tests
**t test here**
```{r}
mean_control_or_diff <- mean(subset_control$trial_or_diff) # -0.52
std_control_or_diff <- sd(subset_control$trial_or_diff) # 0.88

mean_animated_diff <- mean(subset_animated$trial_or_diff) # -0.64
std_animated_or_diff <- sd(subset_animated$trial_or_diff) #0.96


# H0 is that mean diff between control and animated diff = 0: mu=0
t.test(study_data_filtered$trial_or_diff~study_data_filtered$test_group, mu=0, alt="two.sided", conf=0.95)

t.test(data = study_data_filtered, trial_or_diff ~ test_group)



```



*6. Determine the 95% confidence interval for the population mean of each group, and the 95% confidence interval for the difference between the means of any two groups for a variable of your choice.*\

95% confidence interval = point estimate +- 1.96 * standard error
x bar (mean) +- 1.96 * standard error

mean = p

SE = sqrt( p(1-p)/n )  - slides 19-20 of chapter 5

or

SE = SD / sqrt(n)

p = point estimate +/- 1.96 * SE
  = PE +/- ***
We are 95% confident that [PE + x, PE - x] of all animated group patients, PTSD therapy helped them

```{r}
qt(0.025, df = 49) # t*49 = 2.01

# mean +- t * SD/sqrt(n)
# mean +- 2.01 * (SD/sqrt(50))
```

We always have the null and the alternate hypothesis.
The null says no, the experiment didn't make any difference.
Give them animated models - didn't make any difference.
Male or Female, didn't make any difference.
Null = no effect

Alternate - something has happened here that is statistically significant
Though the probability of seeing a difference between pre and post is actually very low
p-value and t test
when p-value is below 0.05 - what you're saying is: I got a sample mean before of x, I got a sample mean of afterwards of x+2
difference is 2 (2/10 e.g.)
p-value below 0.05 means there's only a 5% that the null hypothesis can still be true given that you see these results
5% probability that the null hypothesis is true given that we are seeing the difference in values (of means)
if p-value is so low, and you're seeing the difference in values, and the null hypothesis is true, that's encouraging you to reject the null hypothesis - i.e. < 5% is the probability that the differences in means exist and that the null hypothesis is true
therefore p-value of <0.05 means reject the null hypothesis i.e something is indeed happening

For the CA we have to state what we think is the null (no effect) hypothesis and what the alternate hypothesis is

If it is quite unlikely to obtain results as in the actual data, we should decide to reject the null hypothesis in favour of the alternative

1. start with a null hypothesis (H0) (that represents no effect/the status quo)
2. state the alternative hypothesis (Ha) (the represents our research question)
3. Conduct a hypothesis test under the assumption that the null hypothesis is true, using p-value test which tells us the probability that this has occurred by accident is very low is p-value is < 0.05 i.e. reject null hypothesis - alternative hypothesis is true
so again p-value <0.05 means that there's only a 5% chance that we would be observing the difference between before and after means if the null hypothesis was true. So because the probability is really low, we have to reject the null hypothesis meaning its not true

If the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we must stick with the null hypothesis. i.e. if p-value is >= 0.05, null hypothesis is true

- one null and one alternative hypothesis
- descriptive statistics - plots
- test for normality
- an abstract
- some conclusions, based on p-value reject or accepting null hypothesis

e.g.
mean before is 0.6
mean after is 0.7
the difference between the two means should not be zero, therefore there must be some effect
so that could be our alternate hypothesis
null is that no effect happened

only use student t test



*7. Determine the degree of correlation between any explanatory and response variable of your choice.*\

---

## Magnitude and direction of results
**list results**

---

# Discussion

---

*5. Analyse the data to provide the hypothesis testing conclusion.*\

## Outline findings and relation to the hypothesis

81% of patients experienced improvement in their PTSD symptoms after VR treatment and overall, all test groups experienced an improvement of 13.3% median improvement (-0.72 points) which supports the hypothesis that both animated and static VR therapy reduces PTSD. VR groups reduced PTSD scores by 15.01%, this equals to 3.8% more than the control group (-0.8 compared to -0.54) whihch means that we can also reject the null hypothesis.

VR impact median reduced by -15.01%. Pre score median was 6.16 with a range of 2.06. Post score median was 5.43 with a range of 5.22.

No notable difference between VR groups was experienced (animated versus static) with a 0.49% variance in impact (between pre and post scores)

Control impact median reduced by -11.25%. Pre score median was 5.98 with a range of 4.75. Post score median was 5.31 with a range of 5.44

Gender: no notable difference between gender for those groups recieving VR Therapy (0.08 or 0.49% range in impact) however note that females were also receptive to the control experience (-12.5% impact in control compared to -15.0% in VR)

Note: 70% of the 10 participants who experienced the most impact, were males from the static VR group



1. Determine whether the data provided are appropriate for the test(s) available and whether any data cleaning is required.
- Cleaning of the data was required, specifically; 1 row had to be removed due to a missing test group, 1 row had a mispelt gender which was corrected and 1 row had a CPSS post result of 12 (2 over the maximum scale) however this row was kept in the set as only the OR data was considered in this analysis.
- Is the data appropriate for the tests available? (is 50 big enough for each test group?)

2. Formulate a single hypothesis test to be used to compare the effectiveness of the approaches used during the experiment.

3. Determine if the data meet the assumptions required by any statistical test.

4. Provide descriptive statistics (graphs and tables) for any assumptions made.

5. Analyse the data to provide the hypothesis testing conclusion.

6. Determine the 95% confidence interval for the population mean of each group, and
the 95% confidence interval for the difference between the means of any two groups
for a variable of your choice.

7. Determine the degree of correlation between any explanatory and response variable
of your choice.

## Limitations (if confounding variables are clearly identified by your group)

We're not aware of any confounding variables in the data provided, however there could be limitations to the study/data/other things to consider ...

---

# References

---

- Dooley, M. (2018) 'Jungian Sandplay'. Available at: https://maevedooley.ie/sandplay/. Accessed on 20th December 2022.
- HSE (2022). 'Post-traumatic stress disorder (PTSD)'. Available at: https://www2.hse.ie/conditions/ptsd/treatment/. Accessed on 21 December 2022.
- Mayo Clinic (2022). 'Post-traumatic stress disorder (PTSD)'. Available at: https://www.mayoclinic.org/diseases-conditions/post-traumatic-stress-disorder/symptoms-causes/syc-20355967. Accessed on 21 December 2022.

- [Clean your data with R. R programming for beginners](https://www.youtube.com/watch?v=sV5lwAJ7vnQ&t=2s&ab_channel=RProgramming101)
- [Intro to Confidence Intervals via Proportions](https://www.youtube.com/watch?v=A6_W8qY8zJo&list=PLkIselvEzpM4SHQojH116fYAQJLaN_4Xo&ab_channel=OpenIntroOrg)
- [Hypothesis Testing Fundamentals](https://www.youtube.com/watch?v=NVbPE1_Cbx8&list=PLkIselvEzpM4SHQojH116fYAQJLaN_4Xo&ab_channel=OpenIntroOrg)
- [Inference for Estimators Other Than the Mean](https://www.youtube.com/watch?v=PUMBNtVKr_g&list=PLkIselvEzpM4SHQojH116fYAQJLaN_4Xo&ab_channel=OpenIntroOrg)
- [Why do we so often use 0.05 for hypothesis testing?](https://www.openintro.org/book/stat/why05/)
- [5 1A t distribution](https://www.youtube.com/watch?v=uVEj2uBJfq0&list=PLkIselvEzpM5G3IO1tzQ-DUThsJKQzQCD&ab_channel=Mine%C3%87etinkaya-Rundel)
- [5 1B Inference for a mean](https://www.youtube.com/watch?v=RYVIGj1l4xs&list=PLkIselvEzpM5G3IO1tzQ-DUThsJKQzQCD&ab_channel=Mine%C3%87etinkaya-Rundel)
- [5 2 Inference for paired data](https://www.youtube.com/watch?v=K0QZ9_4w0HU&list=PLkIselvEzpM5G3IO1tzQ-DUThsJKQzQCD&ab_channel=Mine%C3%87etinkaya-Rundel)
- [5 3 Difference of two independent means](https://www.youtube.com/watch?v=emZ24asR2F4&list=PLkIselvEzpM5G3IO1tzQ-DUThsJKQzQCD&ab_channel=Mine%C3%87etinkaya-Rundel)
- [Writing in the sciences...: Lab Reports](https://dkit.ie.libguides.com/writinginthesciences/LabReports)
- [How to Write a Lab Report](https://www.slideshare.net/libhgtc/how-to-write-a-lab-report-45656386)
- [Writing a Laboratory Report original (2016)](https://www.discoveringstatistics.com/docs/writinglabreports.pdf)
- [DPLYR Tutorial: Data Manipulation](https://www.listendata.com/2016/08/dplyr-tutorial.html)
- [How to Test for Normality in R](https://www.statology.org/test-for-normality-in-r/)
- [7 Types of Statistical Analysis: Definition and Explanation](https://www.analyticssteps.com/blogs/7-types-statistical-analysis-definition-explanation)